#!/usr/bin/env python3
"""
Demo script for the Enhanced Chunking System
Shows persistent storage, optimal chunking strategies, and performance improvements.
"""

import os
import time
from pathlib import Path

from parsing.enhanced_parser import EnhancedParser
from retrieval.embedder import EmbeddingModel
from retrieval.enhanced_retriever import EnhancedVectorStore
from utils.config import DOCS_PATH

def demo_enhanced_chunking():
    """Demonstrate the enhanced chunking system."""
    
    print("ğŸš€ Enhanced Chunking System Demo")
    print("=" * 50)
    
    # Initialize components
    print("\n1ï¸âƒ£  Initializing Enhanced Components...")
    parser = EnhancedParser()
    embedder = EmbeddingModel()
    vector_store = EnhancedVectorStore(embedder)
    
    # Show initial cache info
    print("\n2ï¸âƒ£  Initial Cache Status:")
    cache_info = parser.get_cache_info()
    print(f"   ğŸ“ Cache directory: {cache_info['cache_directory']}")
    print(f"   ğŸ“„ Cached files: {cache_info['total_cached_files']}")
    
    # Show initial collection stats
    print("\n3ï¸âƒ£  Initial Collection Status:")
    stats = vector_store.get_collection_stats()
    print(f"   ğŸ“Š Total documents: {stats.get('total_documents', 0)}")
    print(f"   ğŸ“ Document types: {stats.get('document_types', {})}")
    
    # Process documents (first run)
    print("\n4ï¸âƒ£  First Run - Processing Documents...")
    start_time = time.time()
    chunks = parser.load_and_parse_documents()
    first_run_time = time.time() - start_time
    
    if chunks:
        print(f"   â±ï¸  First run time: {first_run_time:.2f} seconds")
        print(f"   ğŸ“„ Chunks generated: {len(chunks)}")
        
        # Add to vector store
        result = vector_store.add_documents(chunks)
        print(f"   âœ… Added to vector store: {result['added']} chunks")
    else:
        print("   âŒ No documents found to process")
        return
    
    # Show cache info after first run
    print("\n5ï¸âƒ£  Cache Status After First Run:")
    cache_info = parser.get_cache_info()
    print(f"   ğŸ“„ Cached files: {cache_info['total_cached_files']}")
    for file_info in cache_info['cached_files']:
        print(f"   ğŸ“‹ {file_info.get('filename', 'Unknown')}: {file_info.get('chunks_count', 0)} chunks")
    
    # Second run - should use cache
    print("\n6ï¸âƒ£  Second Run - Using Cache...")
    start_time = time.time()
    chunks_second = parser.load_and_parse_documents()
    second_run_time = time.time() - start_time
    
    print(f"   â±ï¸  Second run time: {second_run_time:.2f} seconds")
    print(f"   ğŸš€ Speed improvement: {first_run_time/second_run_time:.1f}x faster")
    
    # Show final collection stats
    print("\n7ï¸âƒ£  Final Collection Statistics:")
    stats = vector_store.get_collection_stats()
    print(f"   ğŸ“Š Total documents: {stats.get('total_documents', 0)}")
    print(f"   ğŸ“ Document types: {stats.get('document_types', {})}")
    print(f"   ğŸ“š Sources: {list(stats.get('sources', {}).keys())}")
    print(f"   ğŸ“ Average chunk size: {stats.get('avg_chunk_size', 0):.0f} characters")
    
    # Demonstrate chunking strategies
    print("\n8ï¸âƒ£  Chunking Strategies Used:")
    doc_types = stats.get('document_types', {})
    for doc_type, count in doc_types.items():
        strategy = parser.chunking_strategies.get(doc_type, parser.chunking_strategies["default"])
        print(f"   ğŸ“‹ {doc_type}: {count} chunks (size: {strategy['chunk_size']}, overlap: {strategy['chunk_overlap']})")
    
    print("\nâœ… Demo completed successfully!")
    print("\nğŸ’¡ Key Benefits Demonstrated:")
    print("   â€¢ Persistent chunk storage (no reprocessing)")
    print("   â€¢ Optimal chunking strategies per document type")
    print("   â€¢ Significant performance improvements")
    print("   â€¢ Intelligent document change detection")
    print("   â€¢ Comprehensive metadata tracking")

def demo_chunking_strategies():
    """Demonstrate different chunking strategies."""
    
    print("\nğŸ¯ Chunking Strategies Demo")
    print("=" * 40)
    
    parser = EnhancedParser()
    
    print("\nğŸ“‹ Available Chunking Strategies:")
    for strategy_name, strategy_config in parser.chunking_strategies.items():
        print(f"   ğŸ“„ {strategy_name.upper()}:")
        print(f"      â€¢ Chunk size: {strategy_config['chunk_size']} characters")
        print(f"      â€¢ Overlap: {strategy_config['chunk_overlap']} characters")
        print(f"      â€¢ Separators: {strategy_config['separators'][:2]}...")
    
    print("\nğŸ’¡ Strategy Selection Logic:")
    print("   â€¢ Legal: Smaller chunks for precise clause matching")
    print("   â€¢ Medical: Very small chunks for specific procedures")
    print("   â€¢ Technical: Larger chunks for comprehensive context")
    print("   â€¢ Financial: Medium chunks for policy details")
    print("   â€¢ Default: Balanced approach for general documents")

if __name__ == "__main__":
    try:
        demo_enhanced_chunking()
        demo_chunking_strategies()
    except Exception as e:
        print(f"âŒ Demo failed: {e}")
        print("ğŸ’¡ Make sure you have documents in the data/docs directory") 