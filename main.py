import json
import os
import datetime

from parsing.enhanced_parser import EnhancedParser
from retrieval.embedder import EmbeddingModel
from retrieval.faiss_vector_store import FAISSVectorStore
from generation.enhanced_generator import EnhancedGenerator
from utils.config import DOCS_PATH

def validate_configuration():
    """Validates that all required configuration is in place."""
    print("ğŸ”§ Validating configuration...")
    
    # Check if DOCS_PATH exists
    if not os.path.exists(DOCS_PATH):
        print(f"âŒ Documents directory not found: {DOCS_PATH}")
        print(f"ğŸ’¡ Creating directory: {DOCS_PATH}")
        os.makedirs(DOCS_PATH, exist_ok=True)
    
    # Check if there are any documents
    if not os.listdir(DOCS_PATH):
        print(f"âš ï¸  No documents found in {DOCS_PATH}")
        print("ğŸ’¡ Please add PDF or DOCX files to this directory before running queries.")
        return False
    
    # Check for supported file types
    supported_files = [f for f in os.listdir(DOCS_PATH) 
                      if f.lower().endswith(('.pdf', '.docx')) and not f.startswith('~')]
    
    if not supported_files:
        print(f"âŒ No supported files (PDF/DOCX) found in {DOCS_PATH}")
        return False
    
    print(f"âœ… Found {len(supported_files)} supported documents")
    return True

def setup_pipeline():
    """Initializes and sets up the enhanced RAG pipeline components."""
    print("ğŸš€ Initializing Enhanced RAG Pipeline ---")
    
    # Initialize components
    embedder = EmbeddingModel()
    # Determine embedding dimension from model
    test_embed = embedder.create_embeddings(["test"])
    dim = test_embed.shape[1] if len(test_embed.shape) > 1 else 384
    vector_store = FAISSVectorStore(dim)
    generator = EnhancedGenerator()
    parser = EnhancedParser()
    
    # Get collection statistics
    stats = vector_store.get_collection_stats()
    print(f"ğŸ“Š Current collection: {stats.get('total_documents', 0)} documents")
    
    # Process documents if needed
    if stats.get('total_documents', 0) == 0:
        print("ğŸ“„ Vector store is empty. Processing documents...")
        if not os.listdir(DOCS_PATH):
            print(f"âš ï¸  No documents found in {DOCS_PATH}. Please add policy documents to this directory.")
            return None, None, None

        chunks = parser.load_and_parse_documents()
        if chunks:
            result = vector_store.add_documents(chunks)
            print(f"âœ… Pipeline setup complete: {result['added']} chunks added")
        else:
            print("âŒ No chunks generated. Pipeline setup failed.")
            return None, None, None
    else:
        print("âœ… Using existing processed documents.")
        
        # Show cache info
        cache_info = parser.get_cache_info()
        print(f"ğŸ“ Cached documents: {cache_info['total_cached_files']}")

    print("ğŸ¯ Enhanced Pipeline Ready!")
    return embedder, vector_store, generator

def log_interaction(query, structured_query, retrieved_chunks, final_response):
    """Logs the full query-response interaction for evaluation/debugging."""
    os.makedirs("logs", exist_ok=True)
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")

    log_data = {
        "query": query,
        "structured_query": structured_query.model_dump(),
        "retrieved_chunks": retrieved_chunks,
        "final_response": final_response.model_dump()
    }

    with open(f"logs/{timestamp}.json", "w", encoding="utf-8") as f:
        json.dump(log_data, f, indent=4, ensure_ascii=False)

def main():
    """Main function to run the RAG query process."""
    # Validate configuration first
    if not validate_configuration():
        print("âŒ Configuration validation failed. Please check the setup and try again.")
        return
    
    # Setup pipeline
    embedder, vector_store, generator = setup_pipeline()

    if not all([embedder, vector_store, generator]):
        print("âŒ Pipeline setup failed. Exiting.")
        return

    print("\nğŸš€ RAG System Ready! Type 'exit' to quit.")
    print("ğŸ’¡ Example queries:")
    print("   - 'I need knee surgery, I'm 45 years old male from Mumbai'")
    print("   - 'What's covered for dental procedures?'")
    print("   - 'Is heart surgery covered for a 60-year-old female?'")

    while True:
        try:
            query = input("\nEnter your query (or type 'exit' to quit): ").strip()
            if query.lower() == 'exit':
                print("ğŸ‘‹ Goodbye!")
                break
            elif query.lower() in ['help', '?', 'h']:
                print("\nğŸ“– Available commands:")
                print("   'help' or '?' - Show this help message")
                print("   'exit' - Quit the application")
                print("   'stats' - Show collection statistics")
                print("   'cache' - Show cache information")
                print("   'reprocess' - Force reprocess all documents")
                print("   'api' - Show API status")
                print("   Any other text - Process as a query")
                print("\nğŸ’¡ Example queries:")
                print("   - 'I need knee surgery, I'm 45 years old male from Mumbai'")
                print("   - 'What's covered for dental procedures?'")
                print("   - 'Is heart surgery covered for a 60-year-old female?'")
                continue
            elif query.lower() == 'stats':
                try:
                    stats = vector_store.get_collection_stats()
                    print("\nğŸ“Š Collection Statistics:")
                    print(f"   ğŸ“„ Total documents: {stats.get('total_documents', 0)}")
                    print(f"   ğŸ“ Document types: {stats.get('document_types', {})}")
                    print(f"   ğŸ“š Sources: {list(stats.get('sources', {}).keys())}")
                    print(f"   ğŸ“ Avg chunk size: {stats.get('avg_chunk_size', 0):.0f} chars")
                    print(f"   ğŸ•’ Last updated: {stats.get('last_updated', 'Unknown')}")
                except Exception as e:
                    print(f"âŒ Error getting stats: {e}")
                continue
            elif query.lower() == 'cache':
                try:
                    parser = EnhancedParser()
                    cache_info = parser.get_cache_info()
                    print(f"\nğŸ“ Cache Information:")
                    print(f"   ğŸ“‚ Cache directory: {cache_info['cache_directory']}")
                    print(f"   ğŸ“„ Cached files: {cache_info['total_cached_files']}")
                    for file_info in cache_info['cached_files'][:5]:  # Show first 5
                        print(f"   ğŸ“‹ {file_info.get('filename', 'Unknown')}: {file_info.get('chunks_count', 0)} chunks")
                    if len(cache_info['cached_files']) > 5:
                        print(f"   ... and {len(cache_info['cached_files']) - 5} more files")
                except Exception as e:
                    print(f"âŒ Error getting cache info: {e}")
                continue
            elif query.lower() == 'reprocess':
                try:
                    print("ğŸ”„ Force reprocessing all documents...")
                    parser = EnhancedParser()
                    chunks = parser.load_and_parse_documents(force_reprocess=True)
                    if chunks:
                        result = vector_store.add_documents(chunks, force_reprocess=True)
                        print(f"âœ… Reprocessing complete: {result['added']} chunks added")
                    else:
                        print("âŒ No chunks generated during reprocessing")
                except Exception as e:
                    print(f"âŒ Error during reprocessing: {e}")
                continue
            elif query.lower() == 'api':
                try:
                    api_status = generator.get_api_status()
                    print(f"\nğŸ”Œ API Status:")
                    print(f"   ğŸ¯ Active client: {api_status['active_client']}")
                    print(f"   ğŸ¤– OpenAI available: {api_status['openai_available']}")
                    print(f"   ğŸŒŸ Gemini available: {api_status['gemini_available']}")
                    if api_status['openai_model']:
                        print(f"   ğŸ“‹ OpenAI model: {api_status['openai_model']}")
                    if api_status['gemini_model']:
                        print(f"   ğŸ“‹ Gemini model: {api_status['gemini_model']}")
                except Exception as e:
                    print(f"âŒ Error getting API status: {e}")
                continue
            
            if not query:
                print("âŒ Please enter a valid query.")
                continue

            print(f"\nğŸ”„ Processing query: '{query}'")

            # Step 1: Structured query extraction
            print("\nğŸ“‹ Step 1: Parsing query with LLM...")
            try:
                structured_query = generator.extract_structured_query(query)
                print(f"  âœ… Structured Query: {structured_query.model_dump_json(indent=2)}")
            except Exception as e:
                print(f"  âŒ Error in structured query extraction: {str(e)}")
                print("  ğŸ”„ Continuing with raw query...")
                continue

            # Step 2: Retrieval
            print("\nğŸ” Step 2: Retrieving relevant documents...")
            try:
                retrieved_chunks = vector_store.retrieve_relevant_chunks(query)
                if not retrieved_chunks:
                    print("  âš ï¸  No relevant documents found.")
                    print("  ğŸ’¡ Try rephrasing your question or check if relevant documents are loaded.")
                    continue
                print(f"  âœ… Retrieved {len(retrieved_chunks)} chunks.")
            except Exception as e:
                print(f"  âŒ Error in document retrieval: {str(e)}")
                continue

            # Step 3: Response Generation
            print("\nğŸ¤– Step 3: Generating final response with LLM...")
            try:
                final_response = generator.generate_response(structured_query, retrieved_chunks)
            except Exception as e:
                print(f"  âŒ Error in response generation: {str(e)}")
                continue

            # Step 4: Output and Logging
            print("\nğŸ“„ --- Final Response ---")
            print(final_response.model_dump_json(indent=4))
            print("ğŸ“„ ----------------------")

            # Log interaction
            try:
                log_interaction(query, structured_query, retrieved_chunks, final_response)
                print("  ğŸ“ Interaction logged successfully.")
            except Exception as e:
                print(f"  âš ï¸  Warning: Failed to log interaction: {str(e)}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ Interrupted by user. Goodbye!")
            break
        except Exception as e:
            print(f"\nâŒ Unexpected error: {str(e)}")
            print("ğŸ”„ Please try again or type 'exit' to quit.")

if __name__ == "__main__":
    main()
